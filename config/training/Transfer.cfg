[base]
batch_size=32
epoch_num=100
gpu=0
visdom_ip=127.0.0.1
early_stop_patience=15

[obj]
training_loss=torch.nn.L1Loss()
mask=False
eval_metric=masked_mae,masked_rmse,masked_mape


[optim]
func=torch.optim.Adam
lr=4e-3
weight_decay=2e-5


[scheduler]
func=torch.optim.lr_scheduler.MultiStepLR
gamma=0.5
milestones=[300, 500]

;func=torch.optim.lr_scheduler.CyclicLR
;base_lr=1e-3
;max_lr=2e-3
;cycle_momentum=False
;step_size_up=600

[other]
save_model=False

